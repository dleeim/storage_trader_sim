{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "503027b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd, requests\n",
    "from datetime import date, datetime, timezone, timedelta\n",
    "from urllib.parse import urlencode\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# folders\n",
    "RAW_DIR = \"../data/raw\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# env (for GIE)\n",
    "load_dotenv()\n",
    "GIE_API_KEY = os.getenv(\"GIE_API_KEY\", \"\").strip()\n",
    "\n",
    "def ts():\n",
    "    return datetime.now(tz=timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c830662",
   "metadata": {},
   "source": [
    "# GIE: AGSI (gas storage) raw dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b823fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGSI saved → ../data/raw/agsi_eu_20250820T092136Z.parquet | rows=5344\n"
     ]
    }
   ],
   "source": [
    "AGSI_BASE = \"https://agsi.gie.eu/api\"\n",
    "\n",
    "def agsi_dump_raw(params: dict, out_name: str):\n",
    "    \"\"\"Dump AGSI endpoint with simple paging to Parquet.\"\"\"\n",
    "    assert GIE_API_KEY, \"Set GIE_API_KEY in your .env\"\n",
    "    headers = {\"x-key\": GIE_API_KEY}\n",
    "    frames, page = [], 1\n",
    "    while True:\n",
    "        q = dict(params); q.update({\"page\": page, \"size\": 1000})\n",
    "        r = requests.get(AGSI_BASE, headers=headers, params=q, timeout=60)\n",
    "        if r.status_code == 429:\n",
    "            # gentle retry\n",
    "            import time; time.sleep(65); r = requests.get(AGSI_BASE, headers=headers, params=q, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "        data = payload.get(\"data\", [])\n",
    "        if not data: break\n",
    "        frames.append(pd.DataFrame.from_records(data))\n",
    "        if page >= int(payload.get(\"last_page\", page)): break\n",
    "        page += 1\n",
    "    df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    path = os.path.join(RAW_DIR, f\"{out_name}_{ts()}.parquet\")\n",
    "    df.to_parquet(path, index=False)\n",
    "    print(f\"AGSI saved → {path} | rows={len(df)}\")\n",
    "    return path, df\n",
    "\n",
    "# Examples (pick one or both)\n",
    "# EU aggregate\n",
    "agsi_eu_path, agsi_eu_df = agsi_dump_raw({\"type\": \"eu\"}, out_name=\"agsi_eu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2b69c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALSI saved → ../data/raw/alsi_eu_20250820T092205Z.parquet | rows=4979\n"
     ]
    }
   ],
   "source": [
    "ALSI_BASE = \"https://alsi.gie.eu/api\"\n",
    "\n",
    "def alsi_dump_raw(params: dict, out_name: str):\n",
    "    assert GIE_API_KEY, \"Set GIE_API_KEY in your .env\"\n",
    "    headers = {\"x-key\": GIE_API_KEY}\n",
    "    frames, page = [], 1\n",
    "    while True:\n",
    "        q = dict(params); q.update({\"page\": page, \"size\": 1000})\n",
    "        r = requests.get(ALSI_BASE, headers=headers, params=q, timeout=60)\n",
    "        if r.status_code == 429:\n",
    "            import time; time.sleep(65); r = requests.get(ALSI_BASE, headers=headers, params=q, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "        data = payload.get(\"data\", [])\n",
    "        if not data: break\n",
    "        frames.append(pd.DataFrame.from_records(data))\n",
    "        if page >= int(payload.get(\"last_page\", page)): break\n",
    "        page += 1\n",
    "    df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    path = os.path.join(RAW_DIR, f\"{out_name}_{ts()}.parquet\")\n",
    "    df.to_parquet(path, index=False)\n",
    "    print(f\"ALSI saved → {path} | rows={len(df)}\")\n",
    "    return path, df\n",
    "\n",
    "# EU aggregate\n",
    "alsi_eu_path, alsi_eu_df = alsi_dump_raw({\"type\": \"eu\"}, out_name=\"alsi_eu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d133c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTSOG aggregated (country) saved → data_raw/entsog_aggregated_physical_flow_2025-02-21_2025-08-20.parquet | rows=61655\n"
     ]
    }
   ],
   "source": [
    "import os, time, pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "ENTSOG_BASE = \"https://transparency.entsog.eu/api/v1\"\n",
    "\n",
    "def _daterange_chunks(start_date: date, end_date: date, chunk_days: int):\n",
    "    cur = start_date\n",
    "    while cur <= end_date:\n",
    "        nxt = min(cur + timedelta(days=chunk_days-1), end_date)\n",
    "        yield cur, nxt\n",
    "        cur = nxt + timedelta(days=1)\n",
    "\n",
    "def _read_csv_retry(url, tries=3, sleep=1.5):\n",
    "    for k in range(tries):\n",
    "        try:\n",
    "            return pd.read_csv(url)\n",
    "        except Exception as e:\n",
    "            if k == tries - 1:\n",
    "                raise\n",
    "            time.sleep(sleep * (2**k))\n",
    "\n",
    "def entsog_store_aggregated_physical_flow_csv_chunked(start: str, end: str,\n",
    "                                                      chunk_days: int = 14) -> str:\n",
    "    \"\"\"Robustly pull aggregated country-level Physical Flow with chunking (avoids 502).\"\"\"\n",
    "    start_d = datetime.fromisoformat(start).date()\n",
    "    end_d   = datetime.fromisoformat(end).date()\n",
    "\n",
    "    frames = []\n",
    "    for a, b in _daterange_chunks(start_d, end_d, chunk_days):\n",
    "        params = {\"indicator\": \"Physical Flow\",\n",
    "                  \"from\": a.isoformat(),\n",
    "                  \"to\": b.isoformat(),\n",
    "                  \"periodType\": \"day\",\n",
    "                  \"limit\": 20000}\n",
    "        url = f\"{ENTSOG_BASE}/aggregatedData.csv?{urlencode(params)}\"\n",
    "        df = _read_csv_retry(url, tries=4, sleep=2.0)\n",
    "        if not df.empty:\n",
    "            frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No aggregatedData rows returned across all chunks.\")\n",
    "\n",
    "    # Safe concat + store raw\n",
    "    all_cols = list({c for f in frames for c in f.columns})\n",
    "    frames = [f.reindex(columns=all_cols, copy=False) for f in frames]\n",
    "    df_all = pd.concat(frames, ignore_index=True, copy=False)\n",
    "\n",
    "    out = os.path.join(\"data_raw\", f\"entsog_aggregated_physical_flow_{start}_{end}.parquet\")\n",
    "    df_all.to_parquet(out, index=False)\n",
    "    print(f\"ENTSOG aggregated (country) saved → {out} | rows={len(df_all)}\")\n",
    "    return out\n",
    "END = date.today().isoformat()\n",
    "START = (date.today() - timedelta(days=180)).isoformat()\n",
    "path_country = entsog_store_aggregated_physical_flow_csv_chunked(START, END, chunk_days=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48ceac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteostat HDD (raw temps) saved → ../data/raw/meteostat_hdd_2025-02-21_2025-08-20_20250820T092255Z.parquet | rows=900\n"
     ]
    }
   ],
   "source": [
    "from meteostat import Point, Daily\n",
    "\n",
    "def meteostat_hdd_dump(cities, start_dt, end_dt, base_c=18.0, out_name=\"meteostat_hdd\"):\n",
    "    frames = []\n",
    "    for (name, lat, lon, alt) in cities:\n",
    "        p = Point(lat, lon, alt)\n",
    "        df = Daily(p, start_dt, end_dt).fetch()\n",
    "        if df.empty: \n",
    "            continue\n",
    "        if \"tavg\" not in df.columns:\n",
    "            df[\"tavg\"] = (df.get(\"tmin\") + df.get(\"tmax\")) / 2.0\n",
    "        out = df[[\"tavg\"]].copy()\n",
    "        out[\"city\"] = name\n",
    "        out = out.reset_index().rename(columns={\"time\": \"date\"})\n",
    "        frames.append(out)\n",
    "    panel = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "    path = os.path.join(RAW_DIR, f\"{out_name}_{start_dt.date()}_{end_dt.date()}_{ts()}.parquet\")\n",
    "    panel.to_parquet(path, index=False)\n",
    "    print(f\"Meteostat HDD (raw temps) saved → {path} | rows={len(panel)}\")\n",
    "    return path, panel\n",
    "\n",
    "# Your base cities\n",
    "cities = [\n",
    "    (\"Amsterdam\", 52.37, 4.90, 13),\n",
    "    (\"Berlin\",    52.52, 13.40, 34),\n",
    "    (\"Paris\",     48.86, 2.35, 35),\n",
    "    (\"Milan\",     45.46, 9.19, 120),\n",
    "    (\"Madrid\",    40.42, -3.70, 667),\n",
    "    # add more later if you want\n",
    "]\n",
    "\n",
    "meteostat_path, meteostat_df = meteostat_hdd_dump(\n",
    "    cities,\n",
    "    start_dt = datetime.now() - timedelta(days=180),\n",
    "    end_dt   = datetime.now()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980e0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cfbb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
